{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Movie Rating - Auto Encoder.ipynb",
      "provenance": [],
      "mount_file_id": "1tq3j3gIfPj2IisYAu9vS5MjV_5QO9PhO",
      "authorship_tag": "ABX9TyMTXj69PXXBGPVML9m8sxzG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/forlinarthur/deep-learning-tensorflow/blob/master/Movie_Rating_Auto_Encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyGeXUWrNGyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import parallel\n",
        "from torch.utils import data\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Gwae2pBb4HM",
        "colab_type": "text"
      },
      "source": [
        "# Prepare training and test set \n",
        "- 1 Million registers divided as 20% test and 80% training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FamdcXEYOHsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_set = pd.read_csv(\"drive/My Drive/ML - AZ Course/Data/ml-1m/training_set.csv\")\n",
        "test_set = pd.read_csv(\"drive/My Drive/ML - AZ Course/Data/ml-1m/test_set.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj_6gsnDP42V",
        "colab_type": "code",
        "outputId": "64a9a736-828b-4163-90af-04c46ad54996",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "training_set.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User</th>\n",
              "      <th>Movie</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>661</td>\n",
              "      <td>3</td>\n",
              "      <td>978302109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>914</td>\n",
              "      <td>3</td>\n",
              "      <td>978301968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3408</td>\n",
              "      <td>4</td>\n",
              "      <td>978300275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>2355</td>\n",
              "      <td>5</td>\n",
              "      <td>978824291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1287</td>\n",
              "      <td>5</td>\n",
              "      <td>978302039</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   User  Movie  Rating  Timestamp\n",
              "0     1    661       3  978302109\n",
              "1     1    914       3  978301968\n",
              "2     1   3408       4  978300275\n",
              "3     1   2355       5  978824291\n",
              "4     1   1287       5  978302039"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLs1BOYGcWn9",
        "colab_type": "text"
      },
      "source": [
        "# We need to convert our data into numPy arrays because that's the expected format to be able to transform them into tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSbA833ZP3kV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_set = np.array(training_set, dtype = 'int')\n",
        "test_set = np.array(test_set, dtype = 'int')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORZ2AUO0uksi",
        "colab_type": "code",
        "outputId": "f764a914-6044-4e93-e65c-f5923c111ee9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(len(training_set))\n",
        "print(len(test_set))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "750121\n",
            "250088\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IApb4wXYP-Or",
        "colab_type": "code",
        "outputId": "ce329884-0855-4855-bd64-8833a9efad3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "training_set[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[        1,       661,         3, 978302109],\n",
              "       [        1,       914,         3, 978301968],\n",
              "       [        1,      3408,         4, 978300275],\n",
              "       [        1,      2355,         5, 978824291],\n",
              "       [        1,      1287,         5, 978302039]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KtImeXiNSIb",
        "colab_type": "text"
      },
      "source": [
        "# Get number of users and movies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hSHS9KUOLg7",
        "colab_type": "code",
        "outputId": "cedbdf42-3247-4842-f78c-d7e24788cd11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "nb_users = int(max(training_set[:,0]))\n",
        "nb_movies = int(max(training_set[:,1]))\n",
        "print(f'Number of users: ', nb_users)\n",
        "print(f'Number of movies: ', nb_movies)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of users:  6040\n",
            "Number of movies:  3952\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Qnt_yQ-WZ1n",
        "colab_type": "text"
      },
      "source": [
        "# Convert data into array of users and movies (in order to transform in tensors)\n",
        "- Get all movieId and ratings from each user and create a list of ratings indexed by movieId.\n",
        "- Create a list of lists containing all ratings of each user.\n",
        "- Return the new data list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mzix2HMbOOet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert(data):\n",
        "  new_data = []\n",
        "  for id_users in range(1, nb_users + 1):\n",
        "      id_movies = data[:,1][data[:,0] == id_users]\n",
        "      id_ratings = data[:,2][data[:,0] == id_users]\n",
        "      ratings = np.zeros(nb_movies)\n",
        "      ratings[id_movies - 1] = id_ratings\n",
        "      new_data.append(list(ratings))\n",
        "  return new_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM_Ev8pTf2rM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_set_1 = convert(training_set)\n",
        "test_set_1 = convert(test_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0tqLAkVgo1q",
        "colab_type": "code",
        "outputId": "5402c8fc-d70d-4dd8-9faa-988aff42e069",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(training_set_1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6040"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tHP1PoWXCqj",
        "colab_type": "text"
      },
      "source": [
        "# Convert data into Torch tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SDpsIsdOfhN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "training_set_1 = torch.FloatTensor(training_set_1)\n",
        "test_set_1 = torch.FloatTensor(test_set_1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dfo2VqFglJ3r",
        "colab_type": "code",
        "outputId": "b094d6a2-66cc-4e94-b0ad-6b472982aabd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "training_set_1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[5., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [3., 0., 0.,  ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIjHgBA7XESs",
        "colab_type": "text"
      },
      "source": [
        "# Architecture of Auto-Encoder neural network\n",
        "## Create the fully connected layers containing:\n",
        "* Encoder with a node for each movie (nb_movies) that connects to a 20 nodes layer.\n",
        "* The previous layer will then be connected (shrinked) to a 10 nodes layer.\n",
        "* A Decoder that takes the 10 nodes layer and connects it to a 20 nodes layer.\n",
        "* A last layer that restores the inital movies list size.\n",
        "* Activation function (Sigmoid).\n",
        "* Finally, create a function that will apply the activation over each layer (forward propagation), except the last one (output)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BvtSwijOkLv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class SAE(nn.Module):\n",
        "  def __init__(self, ):\n",
        "    super(SAE, self).__init__()\n",
        "    self.fc1 = nn.Linear(nb_movies, 20)\n",
        "    self.fc2 = nn.Linear(20, 10)\n",
        "    self.fc3 = nn.Linear(10, 20)\n",
        "    self.fc4 = nn.Linear(20, nb_movies)\n",
        "    self.activation = nn.Sigmoid()\n",
        "  def forward(self, x):\n",
        "    x = self.activation(self.fc1(x))\n",
        "    x = self.activation(self.fc2(x))\n",
        "    x = self.activation(self.fc3(x))\n",
        "    x = self.fc4(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tbx1OetSQ2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sae = SAE()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bU-939o3Zk5a",
        "colab_type": "text"
      },
      "source": [
        "#Train Neural Network\n",
        "We will loop over 200 epochs and for each one we take the information from each user and extract it from the tensor, then we check if there are valid ratings (at least one non zero rating), if true we send the data to our sae class. After that we calculate loss and apply a mean corrector. We are now able to call backpropagation, add our loss metric (RMSE) and finally call the RMSprop optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5TPdOSUOqdZ",
        "colab_type": "code",
        "outputId": "40caf486-0cd1-48bf-961b-b75055411178",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train NN\n",
        "nb_epoch = 200\n",
        "for epoch in range(1, nb_epoch + 1):\n",
        "  train_loss = 0\n",
        "  s = 0.\n",
        "  for id_user in range(nb_users):\n",
        "    inpt = Variable(training_set_1[id_user]).unsqueeze(0)\n",
        "    target = inpt.clone()\n",
        "    if torch.sum(target.data > 0) > 0:\n",
        "      output = sae(inpt)\n",
        "      target.require_grad = False\n",
        "      output[target == 0] = 0\n",
        "      loss = criterion(output, target)\n",
        "      mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
        "      loss.backward()\n",
        "      train_loss += np.sqrt(loss.data.item() * mean_corrector)\n",
        "      s += 1.\n",
        "      optimizer.step()\n",
        "  print('epoch: ' + str(epoch) + ' loss: ' + str(train_loss/s))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1 loss: 1.3472274646257396\n",
            "epoch: 2 loss: 1.0100685584508857\n",
            "epoch: 3 loss: 0.9899486759401004\n",
            "epoch: 4 loss: 0.9832039737762297\n",
            "epoch: 5 loss: 0.9801433544190066\n",
            "epoch: 6 loss: 0.9783893666767414\n",
            "epoch: 7 loss: 0.9774148415264873\n",
            "epoch: 8 loss: 0.9765183444392906\n",
            "epoch: 9 loss: 0.9759904665796292\n",
            "epoch: 10 loss: 0.9755635603824311\n",
            "epoch: 11 loss: 0.9752304424767345\n",
            "epoch: 12 loss: 0.9751184393630885\n",
            "epoch: 13 loss: 0.9747343990640828\n",
            "epoch: 14 loss: 0.9745999175112476\n",
            "epoch: 15 loss: 0.9743667555254177\n",
            "epoch: 16 loss: 0.9743899248009193\n",
            "epoch: 17 loss: 0.9741234986375745\n",
            "epoch: 18 loss: 0.9740008441424362\n",
            "epoch: 19 loss: 0.9738845408671623\n",
            "epoch: 20 loss: 0.9739739082931812\n",
            "epoch: 21 loss: 0.9737233644109785\n",
            "epoch: 22 loss: 0.9735608893380282\n",
            "epoch: 23 loss: 0.9734317003837023\n",
            "epoch: 24 loss: 0.9732252147067989\n",
            "epoch: 25 loss: 0.9727567734499101\n",
            "epoch: 26 loss: 0.9719347022547415\n",
            "epoch: 27 loss: 0.9712102859536008\n",
            "epoch: 28 loss: 0.970487125752764\n",
            "epoch: 29 loss: 0.9701063790468102\n",
            "epoch: 30 loss: 0.9689279804717134\n",
            "epoch: 31 loss: 0.9685490949007163\n",
            "epoch: 32 loss: 0.9670427106085656\n",
            "epoch: 33 loss: 0.9670897480225158\n",
            "epoch: 34 loss: 0.9647585307047556\n",
            "epoch: 35 loss: 0.9657489285318576\n",
            "epoch: 36 loss: 0.9645808548954584\n",
            "epoch: 37 loss: 0.9621946875305672\n",
            "epoch: 38 loss: 0.9606559915932978\n",
            "epoch: 39 loss: 0.9589533034896172\n",
            "epoch: 40 loss: 0.9580636457858316\n",
            "epoch: 41 loss: 0.9568078761818092\n",
            "epoch: 42 loss: 0.9579078895610563\n",
            "epoch: 43 loss: 0.9550991105981461\n",
            "epoch: 44 loss: 0.9556243551966056\n",
            "epoch: 45 loss: 0.9572276015118639\n",
            "epoch: 46 loss: 0.9565617595815837\n",
            "epoch: 47 loss: 0.9546007443122826\n",
            "epoch: 48 loss: 0.9503984515507593\n",
            "epoch: 49 loss: 0.9542833078959724\n",
            "epoch: 50 loss: 0.9526101517071411\n",
            "epoch: 51 loss: 0.9493801275002499\n",
            "epoch: 52 loss: 0.9455681767277379\n",
            "epoch: 53 loss: 0.9448258788686679\n",
            "epoch: 54 loss: 0.9440418379271002\n",
            "epoch: 55 loss: 0.9413842213905647\n",
            "epoch: 56 loss: 0.9415621214078131\n",
            "epoch: 57 loss: 0.9416489407020794\n",
            "epoch: 58 loss: 0.9409895096587574\n",
            "epoch: 59 loss: 0.9422300216467656\n",
            "epoch: 60 loss: 0.9406860807431628\n",
            "epoch: 61 loss: 0.9384523640149269\n",
            "epoch: 62 loss: 0.9368552601810183\n",
            "epoch: 63 loss: 0.9360837495888809\n",
            "epoch: 64 loss: 0.9336823856288724\n",
            "epoch: 65 loss: 0.9350357339467031\n",
            "epoch: 66 loss: 0.932718011294129\n",
            "epoch: 67 loss: 0.929018829169314\n",
            "epoch: 68 loss: 0.9288212674377654\n",
            "epoch: 69 loss: 0.924702894590031\n",
            "epoch: 70 loss: 0.9270356010239362\n",
            "epoch: 71 loss: 0.9302028650715467\n",
            "epoch: 72 loss: 0.9256567362541288\n",
            "epoch: 73 loss: 0.9233795198651286\n",
            "epoch: 74 loss: 0.922762034108241\n",
            "epoch: 75 loss: 0.9197518345688415\n",
            "epoch: 76 loss: 0.9213019552872107\n",
            "epoch: 77 loss: 0.926836584368274\n",
            "epoch: 78 loss: 0.9224444668666163\n",
            "epoch: 79 loss: 0.9181358972720389\n",
            "epoch: 80 loss: 0.917431422871128\n",
            "epoch: 81 loss: 0.9167123500189703\n",
            "epoch: 82 loss: 0.9144894490452763\n",
            "epoch: 83 loss: 0.9142021022542608\n",
            "epoch: 84 loss: 0.9136028053461315\n",
            "epoch: 85 loss: 0.9132529576612063\n",
            "epoch: 86 loss: 0.9117581695287977\n",
            "epoch: 87 loss: 0.9147775969159582\n",
            "epoch: 88 loss: 0.9134103279953123\n",
            "epoch: 89 loss: 0.9141243147076571\n",
            "epoch: 90 loss: 0.914304619497574\n",
            "epoch: 91 loss: 0.9143490083589934\n",
            "epoch: 92 loss: 0.9137379260248149\n",
            "epoch: 93 loss: 0.9122179107990479\n",
            "epoch: 94 loss: 0.9098591425605737\n",
            "epoch: 95 loss: 0.9086527718612987\n",
            "epoch: 96 loss: 0.9149665302209206\n",
            "epoch: 97 loss: 0.9164103571066402\n",
            "epoch: 98 loss: 0.9146736027590314\n",
            "epoch: 99 loss: 0.911320006630746\n",
            "epoch: 100 loss: 0.9110390755462859\n",
            "epoch: 101 loss: 0.9151331823091889\n",
            "epoch: 102 loss: 0.9229498395778506\n",
            "epoch: 103 loss: 0.92216550877556\n",
            "epoch: 104 loss: 0.918713679954276\n",
            "epoch: 105 loss: 0.9170384078224414\n",
            "epoch: 106 loss: 0.915606915944628\n",
            "epoch: 107 loss: 0.9147188560316487\n",
            "epoch: 108 loss: 0.9122102946449314\n",
            "epoch: 109 loss: 0.9150210227507684\n",
            "epoch: 110 loss: 0.9136917142224528\n",
            "epoch: 111 loss: 0.9116850063012386\n",
            "epoch: 112 loss: 0.9118754316770273\n",
            "epoch: 113 loss: 0.912539961862225\n",
            "epoch: 114 loss: 0.9106372454201522\n",
            "epoch: 115 loss: 0.9110056914380058\n",
            "epoch: 116 loss: 0.9119390308327872\n",
            "epoch: 117 loss: 0.9077857536324165\n",
            "epoch: 118 loss: 0.9071941072165778\n",
            "epoch: 119 loss: 0.9045929965945513\n",
            "epoch: 120 loss: 0.9022847173025413\n",
            "epoch: 121 loss: 0.9035526498464244\n",
            "epoch: 122 loss: 0.9037217697426997\n",
            "epoch: 123 loss: 0.9030571868578181\n",
            "epoch: 124 loss: 0.9019425260949665\n",
            "epoch: 125 loss: 0.903581022636377\n",
            "epoch: 126 loss: 0.9009216392783767\n",
            "epoch: 127 loss: 0.8995095608345268\n",
            "epoch: 128 loss: 0.8996423662913384\n",
            "epoch: 129 loss: 0.8974698888809618\n",
            "epoch: 130 loss: 0.8965402494043205\n",
            "epoch: 131 loss: 0.8980944601832357\n",
            "epoch: 132 loss: 0.900364221427904\n",
            "epoch: 133 loss: 0.8957972143989428\n",
            "epoch: 134 loss: 0.8939837679613944\n",
            "epoch: 135 loss: 0.8930221411474666\n",
            "epoch: 136 loss: 0.8932931273516093\n",
            "epoch: 137 loss: 0.8944712210491192\n",
            "epoch: 138 loss: 0.897064130360339\n",
            "epoch: 139 loss: 0.8937083344590657\n",
            "epoch: 140 loss: 0.8921906404924074\n",
            "epoch: 141 loss: 0.8928090177025118\n",
            "epoch: 142 loss: 0.8918885222140134\n",
            "epoch: 143 loss: 0.890399760408029\n",
            "epoch: 144 loss: 0.8940834930708842\n",
            "epoch: 145 loss: 0.8957280885725994\n",
            "epoch: 146 loss: 0.8920091271885995\n",
            "epoch: 147 loss: 0.8933546257943988\n",
            "epoch: 148 loss: 0.8982605736438597\n",
            "epoch: 149 loss: 0.8966241516027337\n",
            "epoch: 150 loss: 0.8929088432703389\n",
            "epoch: 151 loss: 0.8914121450954701\n",
            "epoch: 152 loss: 0.8903614727020741\n",
            "epoch: 153 loss: 0.8898842092287413\n",
            "epoch: 154 loss: 0.8892382223578796\n",
            "epoch: 155 loss: 0.8889663957478656\n",
            "epoch: 156 loss: 0.8874577721206082\n",
            "epoch: 157 loss: 0.8874941189156059\n",
            "epoch: 158 loss: 0.8882878230524088\n",
            "epoch: 159 loss: 0.8869697047368558\n",
            "epoch: 160 loss: 0.8863449228491788\n",
            "epoch: 161 loss: 0.8873266082182981\n",
            "epoch: 162 loss: 0.8861846853239024\n",
            "epoch: 163 loss: 0.884633362642846\n",
            "epoch: 164 loss: 0.8836519899748932\n",
            "epoch: 165 loss: 0.883182002671304\n",
            "epoch: 166 loss: 0.8827835075742707\n",
            "epoch: 167 loss: 0.883388948120838\n",
            "epoch: 168 loss: 0.8841853474919353\n",
            "epoch: 169 loss: 0.8819858934235864\n",
            "epoch: 170 loss: 0.8836981575048719\n",
            "epoch: 171 loss: 0.882442849295226\n",
            "epoch: 172 loss: 0.8805944671751863\n",
            "epoch: 173 loss: 0.8810729022911402\n",
            "epoch: 174 loss: 0.8802829807129738\n",
            "epoch: 175 loss: 0.8798297024625814\n",
            "epoch: 176 loss: 0.8813198148905808\n",
            "epoch: 177 loss: 0.8801406692991957\n",
            "epoch: 178 loss: 0.8790442724126144\n",
            "epoch: 179 loss: 0.8802971519471592\n",
            "epoch: 180 loss: 0.8791583069801895\n",
            "epoch: 181 loss: 0.8778374348043362\n",
            "epoch: 182 loss: 0.8789234936301259\n",
            "epoch: 183 loss: 0.8806993224094452\n",
            "epoch: 184 loss: 0.8785424762149797\n",
            "epoch: 185 loss: 0.8765601134328979\n",
            "epoch: 186 loss: 0.8771466556780503\n",
            "epoch: 187 loss: 0.8775997619519947\n",
            "epoch: 188 loss: 0.8772052753370546\n",
            "epoch: 189 loss: 0.8763212873909558\n",
            "epoch: 190 loss: 0.8772796581255198\n",
            "epoch: 191 loss: 0.8765148213908311\n",
            "epoch: 192 loss: 0.8757380334489999\n",
            "epoch: 193 loss: 0.8766575710719808\n",
            "epoch: 194 loss: 0.8752588844958958\n",
            "epoch: 195 loss: 0.8741762258904494\n",
            "epoch: 196 loss: 0.8743126279074389\n",
            "epoch: 197 loss: 0.873718167971619\n",
            "epoch: 198 loss: 0.8733387248885709\n",
            "epoch: 199 loss: 0.8730265321204833\n",
            "epoch: 200 loss: 0.8728275413825726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0JeybRHoqQo",
        "colab_type": "text"
      },
      "source": [
        "Training over 1M - epoch: 200 loss: 0.8728275413825726"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF2fs0kFbkJk",
        "colab_type": "text"
      },
      "source": [
        "# Test NN (final result)\n",
        "For testing the network we do almost the same as we did for training except the epochs looping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKdrzH-pOs6k",
        "colab_type": "code",
        "outputId": "4e4ed367-f10d-4098-93f1-1a43479dd791",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "test_loss = 0\n",
        "s = 0.\n",
        "for id_user in range(nb_users):\n",
        "    inpt = Variable(training_set_1[id_user]).unsqueeze(0)\n",
        "    target = Variable(test_set_1[id_user]).unsqueeze(0)\n",
        "    if torch.sum(target.data > 0) > 0:\n",
        "        output = sae(inpt)\n",
        "        target.require_grad = False\n",
        "        output[target == 0] = 0\n",
        "        loss = criterion(output, target)\n",
        "        mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
        "        test_loss += np.sqrt(loss.data.item() * mean_corrector)\n",
        "        s += 1.\n",
        "print('Test loss: ' + str(test_loss/s))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.9083699782989956\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeDfIzfvow74",
        "colab_type": "text"
      },
      "source": [
        "Test over 1M - Test loss: 0.9083699782989956"
      ]
    }
  ]
}